# Databricks notebook source
import os
import numpy as np
import pandas as pd

from pyspark import SparkFiles
from pyspark import SparkContext
from pyspark.sql import functions
import pyspark.sql.functions #import avg, col, udf
from pyspark.sql import SQLContext
from pyspark.sql import DataFrame
from pyspark.sql.types import *
import json

path = '/dbfs/FileStore/tables/FirstRate30mins'
filename_lists = os.listdir(path)
df_30mins_ = {}
_delta ={}

for filename in os.listdir(path):
    name = filename.split('_')[0]
    temp = StructType([StructField(name+"_date/time", StringType(), True),StructField(name+"_adjOpen", FloatType(), True),StructField(name+"_adjHigh", FloatType(), True),StructField(name+"_adjLow", FloatType(), True),StructField(name+"_adjClose", FloatType(), True),StructField(name+"_adjVolume", IntegerType(), True)])
    
    temp_delta = StructType([StructField(name+"_30mins_delta", StringType(), True)])
    
    print(temp_delta)
    
    temp_df = spark.read.format("csv").option("header", "false").schema(temp).load("/FileStore/tables/FirstRate30mins/")
   
    df_30mins_[name] = temp_df
    
    table_name = name+'_30mins_delta'
        
    df_30mins_[name].write.format("delta").mode("overwrite").saveAsTable(table_name)


# COMMAND ----------

display(df_30mins_['AAL'])
display(spark.sql('DESCRIBE aal_30mins_delta'))